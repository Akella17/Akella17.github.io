<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.55.6" />
  <meta name="author" content="Akella Ravi Tej">
  <meta name="description" content="&lt;first_name&gt;@cs.toronto.edu">

  <link rel="stylesheet" href="/css/highlight.min.css">
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/academicons.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono">
  <link rel="stylesheet" href="/css/hugo-academic.css">
  

  <link rel="alternate" href="https://akella17.github.io/index.xml" type="application/rss+xml" title="Akella Ravi Tej">
  <link rel="feed" href="https://akella17.github.io/index.xml" type="application/rss+xml" title="Akella Ravi Tej">

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="https://akella17.github.io/blog/Bayesian-Quadrature-for-Policy-Gradient/">

  <title>Bayesian Quadrature for Policy Gradient | Akella Ravi Tej</title>

</head>
<body id="top">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Akella Ravi Tej</a>
    </div>

    
    <div class="collapse navbar-collapse" id="#navbar-collapse-1">

      
      <ul class="nav navbar-nav navbar-right">
        
        <li class="nav-item"><a href="/#">Home</a></li>
        
        <li class="nav-item"><a href="/#publications">Publications</a></li>
        
        <li class="nav-item"><a href="/#Blogs">Blogs</a></li>
        
        <li class="nav-item"><a href="/#News">News</a></li>
        
      </ul>

    </div>
  </div>
</nav>

<div class="container">

<div class="pub" itemscope itemtype="http://schema.org/CreativeWork">
    <div class="pub-title">
      <br>
      <h1 itemprop="name" style="font-size: 275%; text-align: center; font-variant: small-caps; font-family: Comic Sans MS; color: #008080"><b>Bayesian Quadrature for Policy Gradient Estimation</b></h1>

      <p class="pub-authors" itemprop="author" align="center">
        
        <h2 style="font-size: 175%; text-align: center; font-variant: small-caps; color: #893a5d"><b> Accuracy &uarr; &emsp; &emsp; Variance &darr; &emsp; &emsp; Sample Efficiency &uarr; &emsp; &emsp; Reliability &uarr; </b></h2>
</p><p align="center"><b><i>Published on 25<sup>th</sup> Oct, 2020</i></b></p><br>
</div>

<p><i>
	In this post, I share my views on the Bayesian Quadrature method for policy gradient estimation. In particular, this blog is only a subset of my findings from my recent colaboration with <a href="https://www.cs.purdue.edu/homes/kamyar/">Kamyar Azizzadenesheli</a> (Purdue University), <a href="https://mohammadghavamzadeh.github.io/">Mohammad Ghavamzadeh</a> (Google Research), <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a> (Caltech & NVIDIA), and <a href="http://www.yisongyue.com/">Yisong Yue</a> (Caltech). For full details, do check-out our <a href="https://arxiv.org/abs/2006.15637">Deep Bayesian Quadrature Policy Optimization</a> paper and its <a href="https://github.com/Akella17/Deep-Bayesian-Quadrature-Policy-Optimization">open source implementation</a>. 
	<!-- Further, the only reason I use &ldquo;I&rdquo; instead of &ldquo;we&rdquo; throughout the blog is to ensure that I am responsible for any mistakes.  -->
</i></p>
<br>
<h1 style="text-align: center; font-variant: small-caps;">TL; DR</h1>
<br>
<p>
	Bayesian Quadrature method provides a way to approximate numerical integration under a probability distribution using samples drawn from the same distribution. As it happens, policy gradient is also an intractable integral equation which is generally approximated using samples. That's pretty much it! BQ can be used for approximating PG.
</p>

<p>
	<b><i>But, why exactly do we need BQ-PG? : </i></b> While it may not seem super exciting at the first glace, BQ-PG has several interesting properties, including connections with Monte-Carlo (MC) estimation (popular choice) and uncertainty quantification. But even if theory does not interest you, this approach empirically offers (i) more accurate policy gradient estimates with a significantly lower variance, and (ii) a consistent imporvement in sample complexity and average return across $7$ MuJoCo benchmark environments. So the question really is <i>&ldquo;Why not BQ-PG?&rdquo;</i>.
</p>

<br>
<h1 style="text-align: center; font-variant: small-caps;">Problem Formulation</h1>
<br>

<div id="header" style="height:15%;width:100%;">
    <div style='float:left; width:50%; padding: 5px;'>
        <img src="https://akella17.github.io/img/RL/MDP_drawio.png" class="pub-banner" itemprop="image">
        <!-- <img src="/e-com/images/logo.jpg" style="margin-left:15%;margin-top:5%"/> -->
    </div>
    <div style='float:left; width:50%; padding: 5px;'>
        <!-- <table border="1" width="44" style="margin-left:30%;float:top;">  -->
        <table style="text-align: center; border: 1px solid black; border-left:1px solid black;">
          <tr>
            <td>State-Space</td>
            <td style="border-left : 1px solid black;">$s_t \in \mathcal{S}$</td>
          </tr>
          <tr>
            <td>Action-Space</td>
            <td style="border-left : 1px solid black;">$a_t \in \mathcal{A}$</td>
          </tr>
          <tr>
            <td>Transition Kernel</td>
            <td style="border-left : 1px solid black;">$P : \mathcal{S} \times \mathcal{A} \rightarrow \Delta_\mathcal{S}$</td>
          </tr>
          <tr>
            <td>Reward Kernel</td>
            <td style="border-left : 1px solid black;">$r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$</td>
          </tr>
          <tr>
            <td>Initial State Distribution</td>
            <td style="border-left : 1px solid black;">$\rho_0:\mathcal{S} \rightarrow \Delta_\mathcal{S}$</td>
          </tr>
          <tr>
            <td>Stochastic Policy</td>
            <td style="border-left : 1px solid black;">$\pi_{\theta}: \mathcal{S} \rightarrow \Delta_\mathcal{A}$</td>
          </tr>
        </table>
    </div>
</div>

<p>
	Consider a Markov decision process (MDP) $\langle\mathcal{S}, \mathcal{A}, P, r, \rho_0, \gamma\rangle$, where $\mathcal{S}$ is the state-space, $\mathcal{A}$ is the action-space, $P : \mathcal{S} \times \mathcal{A} \rightarrow \Delta_\mathcal{S}$ is the transition kernel that maps each state-action pair to a distribution over the states $\Delta_\mathcal{S}$, $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the reward kernel, $\rho_0:\mathcal{S} \rightarrow \Delta_\mathcal{S}$ is the initial state distribution, and $\gamma \in [0,1)$ is the discount factor. We denote by $\pi_{\theta}: \mathcal{S} \rightarrow \Delta_\mathcal{A}$ a stochastic parameterized policy with parameters $\theta\in\Theta$. The MDP controlled by the policy $\pi_{\theta}$ induces a Markov chain over state-action pairs $z = (s,a) \in \mathcal{Z} = \mathcal{S} \times \mathcal{A}$, with an initial density $\rho^{\pi_{\theta}}_0(z_0) = \pi_{\theta}(a_0|s_0) \rho_0(s_0)$ and a transition probability distribution $P^{\pi_{\theta}}(z_t|z_{t-1}) = \pi_{\theta}(a_t|s_t) P(s_t|z_{t-1})$. The discounted state-action visitation frequency $\rho^{\pi_{\theta}}$ is defined as:

	\begin{equation}
	\label{eqn:state_visitation}
	P^{\pi_{\theta}}_t(z_t) = \int_{\mathcal{Z}^t} dz_0 ... dz_{t-1} P^{\pi_{\theta}}_0(z_0) \prod_{\tau=1}^t P^{\pi_{\theta}}(z_\tau|z_{\tau-1}), \qquad \rho^{\pi_{\theta}}(z) = \sum_{t=0}^{\infty} \gamma^t P^{\pi_{\theta}}_t(z).
	\end{equation}

	I will be using the standard definition of action-value function $Q_{\pi_{\theta}}$:
	
	\begin{equation}
	\label{eqn:q_func_definitions}
	Q_{\pi_{\theta}} (z_t) = \: \mathbb{E}\Big[\sum\nolimits_{\tau=0}^{\infty} \gamma^\tau r(z_{t+\tau}) ~\Big|~ z_{t+\tau+1} \sim P^{\pi_{\theta}}(.|z_{t+\tau})\Big].
	\end{equation}
	
	In simple words, $Q_{\pi_{\theta}} (z)$ is the expected sum of (discounted) rewards along a trajectory starting from $z = (s,a)$ and jointly-controlled by the agent's policy $\pi_{\theta}$ (<span style="color:limegreen">known</span>) and MDP's dynamics $\langle\rho_0, r, P \rangle$ (<span style="color:red">unknown</span>). This notation allows us to conveniently formulate the expected return $J(\theta)$ under $\pi_{\theta}$ simply as the average $Q_{\pi_{\theta}}$ value of MDP's initial states:
	
	\begin{equation}
	\label{eqn:old_pg_objective_definitions}
	J(\theta) = \mathbb{E}_{z_0\sim\rho^{\pi_{\theta}}_0}\left[Q_{\pi_{\theta}}(z_0)\right] = \mathbb{E}_{s_0\sim\rho_0}\left[ \mathbb{E}_{a_0\sim \pi_{\theta}(.|s_0)} \left[ Q_{\pi_{\theta}}(s_0, a_0) \right] \right].
	\end{equation}

	However, the gradient of $J(\theta)$ with respect to policy parameters $\theta$ cannot be directly computed from this formulation. The policy gradient theorem (<a href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">Sutton et al. 2000</a>) provides an analytical expression for the gradient of the expected return $J(\theta)$, as:
	
	\begin{equation}
	\label{eqn:policy_gradient_theorem}
	\displaystyle \nabla_{\theta} J(\theta) = \int_{\mathcal{Z}} \rho^{\pi_\theta}(z) \nabla_{\theta} \log \pi_{\theta} (a|s) Q_{\pi_\theta}(z) dz = \mathbb{E}_{z \sim \rho^{\pi_\theta}}\big[\nabla_{\theta} \log \pi_{\theta} (a|s) Q_{\pi_\theta}(z)\big],
	\end{equation}
	
	<b>Note</b> that the initial state distribution $\rho_0(s)$, initial state-action density $\rho^{\pi_{\theta}}_0(z)$, and the discounted state-action visitation frequency $\rho^{\pi_{\theta}} (z)$ have different definitions. Nevertheless, the exact evaluation of the PG integral in Eq. \ref{eqn:policy_gradient_theorem} is often intractable for MDPs with a large (or continuous) state or action space. This is because $Q_{\pi_{\theta}} (z)$ and $\rho^{\pi_{\theta}}$ cannot be computed exactly due to their dependence on the MDP's <span style="color:red">unknown</span> dynamics. $Q_{\pi_{\theta}} (z)$ is generally computed from Monte-Carlo policy rollouts (a.k.a TD($1$) estimates), function approximation (e.g. TD($0$), TD($\lambda$) estimates), or a hybrid of both, i.e. advantage function (e.g. <a href="https://arxiv.org/abs/1602.01783">A2C</a>, <a href="https://arxiv.org/abs/1506.02438">GAE</a>). While $\rho^{\pi_{\theta}}$ is unknown, sampling from $\rho^{\pi_{\theta}}$ can be simulated by running the policy $\pi_{\theta}$ in the environment. Thus, in practice, the PG integral needs to be approximated using a finite set of samples $\{z_i\}_{i=1}^{n} \sim \rho^{\pi_\theta}$.

</p>

<br>
<h1 style="text-align: center; font-variant: small-caps;">Monte-Carlo Method: Predominant Choice for PG Estimation</h1>
<br>
<p>
	<b>Monte-Carlo (MC)</b> method approximates the PG integral in Eq \ref{eqn:policy_gradient_theorem} by the finite sum:

	\begin{equation}
	\label{eqn:Monte-Carlo_PG}
	\displaystyle \int_{\mathcal{Z}} \rho^{\pi_\theta}(z) \mathbf{u}(z) Q_{\pi_\theta}(z) dz \; \approx \; \mathbf{L}_{\theta}^{MC} = \frac{1}{n}\sum_{i=1}^{n} Q_{\pi_\theta}(z_i) \mathbf{u}(z_i) = \frac{1}{n} \mathbf{U} \mathbf{Q},
	\end{equation}

	where $\mathbf{u}(z) = \nabla_{\theta} \log \pi_{\theta} (a|s)$ is the score function, a $|\Theta|$ dimensional vector ($|\Theta|$ is the number of policy parameters). To keep the expressions concise, the function evaluations at sample locations $\{z_i\}_{i=1}^{n} \sim \rho^{\pi_\theta}$ are grouped into $\mathbf{U} = [ \mathbf{u}(z_1), ..., \mathbf{u}(z_n)]$, a $|\Theta| \times n$ dimensional matrix, and $\mathbf{Q} = [Q_{\pi_\theta}(z_1),...,Q_{\pi_\theta}(z_n)]$, an $n$ dimensional vector. Here, $Q_{\pi_\theta}(z)$ is estimated from MC rollouts or function approximation.<br><br>

  One disadvantage of MC-PG is that it returns stochastic gradient estimates with non-uniform gradient uncertainty, i.e., the MC estimator is more uncertain about the stepsize of some gradient components than others. Thus, MC-PG occasionally take large steps along directions of high uncertainty, resulting in a bad policy update and catastrophic performance degradation (<b><span style="color: #893a5d">reliability &darr;</span></b>). In addition, MC-PG is a point estimation method and hence does not quantify the uncertainty in its gradient estimation. Thus, the only way to make MC-PG estimates more robust to large policy updates is by increasing the number of samples (<b><span style="color: #893a5d">sample efficiency &darr;</span></b>).<br><br>

	On the other hand, MC method returns the gradient mean evaluated at sample locations, which, according to the central limit theorem (CLT), is an unbiased estimate of the true gradient. However, CLT also suggests that the MC estimates suffer from a slow convergence rate ($n^{-1/2}$) (<b><span style="color: #893a5d">accuracy &darr;</span></b>) and high variance (<b><span style="color: #893a5d">variance &uarr;</span></b>), necessitating a large sample size $n$ (<b><span style="color: #893a5d">sample efficiency &darr;</span></b>) for reliable PG estimation (<a href="https://arxiv.org/abs/1811.02553">Ilyas et al., 2018</a>) (<b><span style="color: #893a5d">reliability &darr;</span></b>). Yet, MC method is significantly more computationally efficient than its sample-efficient alternatives (like BQ), making it the predominantly used PG estimator in most deep PG algorithms (e.g. <a href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">Vanilla PG</a>, <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">NPG</a>, <a href="https://arxiv.org/abs/1502.05477">TRPO</a>).

</p>

<br>
<h1 style="text-align: center; font-variant: small-caps;">Bayesian Quadrature</h1>
<br>
<p>
  <b>Bayesian quadrature (BQ)</b> (<a href="https://www.sciencedirect.com/science/article/abs/pii/037837589190002V">O'Hagan, 1991</a>) is an approach from <i>probabilistic numerics</i> (<a href="https://royalsocietypublishing.org/doi/10.1098/rspa.2015.0142">Hennig et al., 2015</a>) that converts numerical integration into a Bayesian inference problem. But before we dive into the BQ method, let me first informally introduce the idea behind it:
  
  <table>
    <tr>
      <td style="text-align: center;"><b>Step 1:</b></td>
      <td style="text-align: left;"><i>Choose</i> a function approximation to model the integrand (the function you wish to integrate).</td>
    </tr>
    <tr>
      <td style="text-align: center;"><b>Step 2:</b></td>
      <td style="text-align: left;">Learn a <i>good approximation</i> of the integrand by fitting this model to sample data.</td>
    </tr>
    <tr>
      <td style="text-align: center;"><b>Step 3:</b></td>
      <td style="text-align: left;">Substitute the actual integrand with the learned function approximation model, and solve the integral in <i>closed-form</i>.</td>
    </tr>
  </table>

  Okay, I might have oversimplified BQ! While <b>Steps 1-2</b> are fairly general, <b>Step 3</b> only supports select few classes of function approximators. For instance, black-box function approximators such as neural networks cannot be integrated over their inputs in closed-form. In the following paragraphs, I will brief the algorithm-design choices in BQ that allow us to obtain a closed-form solution to the PG integral equation.
</p>
<p>
  <b>Step 1:</b> <i>Formulate a prior stochastic model over the integrand.</i><br>
  In the case of a PG integral, the integrand is the action-value function $Q_{\pi_\theta}$ and the function approximator is a Gaussian process (GP), a non-parametric model that is defined using a prior mean and covariance function. More specifically, the GP prior placed on the $Q_{\pi_\theta}$ function has zero mean $\mathbb{E}\left[ Q_{\pi_\theta}(z) \right] = 0$, a prior covariance function $k(z_p,z_q) = \text{Cov} [Q_{\pi_\theta}(z_p),Q_{\pi_\theta}(z_q)]$, and observation noise $\sigma$. This is a common prior in BQ with many appealing properties, such as the joint distribution over any finite number of action-values (indexed by the state-action inputs, $z \in \mathcal{Z}$) is Gaussian:
  
  \begin{equation}
  \label{eqn:appendix_Q_function_prior}
  \mathbf{Q} = [Q_{\pi_\theta}(z_1),...,Q_{\pi_\theta}(z_n)] \sim \mathcal{N}(0,\mathbf{K}), \text{where } \mathbf{K} \text{ is the Gram matrix with entries } \mathbf{K}_{p,q} = k(z_p,z_q).
  \end{equation}

  <b>Step 2:</b> <i>This GP prior is conditioned (Bayes rule) on the observed samples $\; \mathcal{D} = \{z_i\}_{i=1}^{n}$ to obtain the posterior moments of $\; Q_{\pi_\theta}$:</i><br>

  \begin{align}
  \mathbb{E}\left[ Q_{\pi_\theta}(z)| \mathcal{D} \right] &= \mathbf{k}(z)^\top (\mathbf{K} +\sigma^2\mathbf{I})^{-1} \mathbf{Q},\nonumber
  \\
  \text{Cov}\left[ Q_{\pi_\theta}(z_1), Q_{\pi_\theta}(z_2)| \mathcal{D} \right] &= k  (z_1,z_2) - \mathbf{k}(z_1)^\top (\mathbf{K} + \sigma^2\mathbf{I})^{-1} \mathbf{k}(z_2), \label{eqn:appendix_Q_function_posterior}
  \\
  \text{where } \mathbf{k}(z) = [k (z_1,z &), ..., k(z_n, z)], \; \mathbf{K} = [\mathbf{k}(z_1), ..., \mathbf{k}(z_n)]. \nonumber
  \end{align}

  To clarify, $k(z, z')$ is a scalar, $\mathbf{k}(z)$ is an $n$ dimensional vector, $\mathbf{K}$ is an $n \times n$ dimensional matrix, and $\mathbf{Q}$ is an $n$ dimensional vector (same as in MC-PG). In Eq. \ref{eqn:appendix_Q_function_posterior}, the GP model approximates the $Q_{\pi_\theta}$ function (mean) and the uncertainty in its estimation (covariance), in a way that simplifies <b>Step 3</b>.<br><br>

  <b>Step 3:</b> <i>Since the transformation from $\; Q_{\pi_\theta}(z)$ to $\; \nabla_{\theta} J(\theta)$ happens through a linear integral operator </i>(Eq. \ref{eqn:policy_gradient_theorem})<i>, the posterior moments of $\; Q_{\pi_\theta}$ can be used to compute a Gaussian approximation of $\;\nabla_{\theta} J(\theta)$:</i>

  \begin{align}
      & \quad \qquad \displaystyle \mathbf{L}_{\theta}^{BQ} = \mathbb{E} \left[ \nabla_{\theta} J(\theta) | \mathcal{D} \right] =\displaystyle \int \rho^{\pi_\theta}(z) \mathbf{u}(z) \mathbb{E} \left[ Q_{\pi_\theta}(z) | \mathcal{D} \right] dz \nonumber
      \\
      &  \quad \qquad \qquad = \displaystyle \left( \int \rho^{\pi_\theta}(z) \mathbf{u}(z) \mathbf{k}(z)^\top dz\right) (\mathbf{K} +\sigma^2\mathbf{I})^{-1}\mathbf{Q} \label{eqn:appendix_BayesianQuadratureIntegral}
      \\
      \displaystyle \mathbf{C}_{\theta}^{BQ} &= \text{Cov}[ \nabla_{\theta} J( \theta) | \mathcal{D}] = \displaystyle \int \rho^{\pi_\theta}(z_1) \rho^{\pi_\theta}(z_2) \mathbf{u}(z_1) \text{Cov}[ Q_{\pi_\theta}(z_1), Q_{\pi_\theta}(z_2)| \mathcal{D}] \mathbf{u}(z_2)^\top dz_1 dz_2,\nonumber
      \\
      &= \displaystyle \int \rho^{\pi_\theta}(z_1) \rho^{\pi_\theta}(z_2) \mathbf{u}(z_1) \Big( k(z_1,z_2) - \mathbf{k}(z_1)^\top (\mathbf{K} +\sigma^2\mathbf{I})^{-1} \mathbf{k}(z_2) \Big) \mathbf{u}(z_2)^\top dz_1 dz_2,\nonumber
  \end{align}

  where the posterior mean $\mathbf{L}_{\theta}^{BQ}$ can be interpreted as the PG estimate and the posterior covariance $\mathbf{C}_{\theta}^{BQ}$ quantifies the uncertainty in PG estimation. However, we still do not have a closed-form solution to PG because the integrals in  Eq. \ref{eqn:appendix_BayesianQuadratureIntegral} cannot be solved analytically for an arbitrary GP prior covariance function $k(z_1, z_2)$. <a href="https://icml.cc/imls/conferences/2007/proceedings/papers/458.pdf">Ghavamzadeh & Engel (2007)</a> showed that these integrals have analytical solutions when the GP kernel $k$ is the additive composition of an arbitrary state kernel $k_s$ and the (indispensable) Fisher kernel $k_f$:
  
  \begin{align}
  \label{eqn:appendix_kernel_definitions}
      k(z_1,z_2) = c_1 k_s(s_1,s_2) + c_2 k_f(z_1,z_2), \quad k_f(z_1,z_2) &= \mathbf{u}(z_1)^\top \mathbf{G}^{-1} \mathbf{u}(z_2),
  \end{align}

  where $\mathbf{G}$ is the Fisher information matrix of the policy $\pi_\theta$. Note that $c_1, c_2$ are redundant hyperparameters that are simply introduced for better explanation of BQ-PG. Now, using the following definitions:

  \begin{eqnarray}
  \label{eqn:appendix_kernel_equations}
  \mathbf{k}_f(z) = \mathbf{U}^\top\mathbf{G}^{-1} \mathbf{u}(z), \ \ \mathbf{K}_f = \mathbf{U}^\top \mathbf{G}^{-1} \mathbf{U}, \ \ \mathbf{K} = c_1 \mathbf{K}_s + c_2 \mathbf{K}_f, \ \ 
  \mathbf{G} = \mathbb{E}_{z \sim \rho^{\pi_\theta}} [\mathbf{u}(z) \mathbf{u}(z)^\top] \approx \frac{1}{n} \mathbf{U}\mathbf{U}^\top,
  \end{eqnarray}

  the closed-form expressions for PG estimate $\mathbf{L}_{\theta}^{BQ}$ and its uncertainty $\mathbf{C}_{\theta}^{BQ}$ are obtained as follows,
  <a class="btn btn-outline btn-warning btn-xs",
  onclick="
      var div = document.getElementById('DBQPG_proof');
      if (div.style.display !== 'none') {
          div.style.display = 'none';
      }
      else {
          div.style.display = 'block';
      }
  ">
    <b>&larr; Proof &rarr;</b>
  </a>
  
  <div id = "DBQPG_proof", class="boxed", style="border: 1px solid green;padding: 10px;background-color:#fed8b1;display:none">
    <h2 style="text-align: center;"><b>Useful Identities</b></h2>
    Expectation of the score vector $\mathbf{u}(z) = \nabla_{\theta} \log \pi_{\theta}(a|s)$ under the policy distribution $\pi_{\theta}(a|s)$ is $\mathbf{0}$:
    
    \begin{align}
    \mathbb{E}_{a \sim \pi_\theta(.|s)} \left[ \mathbf{u}(z) \right] &= \mathbb{E}_{a \sim \pi_{\theta}(.|s)} \left[ \nabla_{\theta} \log \pi_{\theta}(a|s) \right] = \int \pi_{\theta}(a|s) \nabla_{\theta} \log \pi_{\theta}(a|s) da \nonumber
    \\
    &= \int \pi_{\theta}(a|s) \frac{\nabla_{\theta} \pi_{\theta}(a|s)}{\pi_{\theta}(a|s)} da = \int \nabla_{\theta} \pi_{\theta}(a|s) da \nonumber
    \\
    &= \nabla_{\theta} \left( \int \pi_{\theta}(a|s) da \right) = \nabla_{\theta} (1) = \mathbf{0} \label{eqn:appendix_logprob_expectiation} \nonumber
    \end{align}
    
    This identify can further be used to derive the following results:

    \begin{equation}
    \mathbb{E}_{a \sim \pi_\theta(.|s)} \left[ k_s(s, s') u(z) \right] = 0, \qquad \mathbb{E}_{a \sim \pi_\theta(.|s)} \left[ \mathbf{k}_s(s) u(z) \right] = 0 \nonumber
    \end{equation}

    <!-- \begin{align}
    \label{eqn:appendix_fisher_expectation}
    \begin{split}
    \mathbb{E}_{a \sim \pi_\theta(.|s)} \left[ k_f(z,z') \right] &= \mathbb{E}_{a \sim \pi_\theta(.|s)} \left[ \mathbf{u}(z)^\top \mathbf{G}^{-1} \mathbf{u}(z') \right] = \mathbb{E}_{a \sim \pi_\theta(.|s)} \left[ \mathbf{u}(z)^\top \right]  \mathbf{G}^{-1} \mathbf{u}(z') = 0
    \end{split}\nonumber
    \end{align} -->
    <br>
    <h2 style="text-align: center;"><b>BQ-PG Proof</b></h2>
    \begin{align}
    \label{eqn:appendix_pg_mean_closedform}
    \begin{split}
        \displaystyle \mathbf{L}_{\theta}^{BQ} &= \mathbb{E} \left[ \nabla_{\theta} J(\theta) | \mathcal{D} \right] = \displaystyle \int \rho^{\pi_\theta}(z) \mathbf{u}(z) \mathbb{E} \left[ Q_{\pi_\theta}(z) | \mathcal{D} \right] dz
        \\
        &= \mathbb{E}_{z \sim \rho^{\pi_\theta}} \left[ \mathbf{u}(z) \mathbb{E} [Q_{\pi_\theta}(z) | \mathcal{D}] \right] 
        \\
        &= \mathbb{E}_{z \sim \rho^{\pi_\theta}} \left[ \mathbf{u}(z) \mathbf{k}(z)^\top \right] (\mathbf{K} +\sigma^2\mathbf{I})^{-1}\mathbf{Q}
        \\
        &= \left( c_1 E_{z \sim \rho^{\pi_\theta}} \left[ \mathbf{u}(z) \mathbf{k}_s(s)^\top \right] + c_2 E_{z \sim \rho^{\pi_\theta}} \left[ \mathbf{u}(z) \mathbf{k}_f(z)^\top \right] \right) (\mathbf{K} +\sigma^2\mathbf{I})^{-1}\mathbf{Q}
        \\
        &= c_2 E_{z \sim \rho^{\pi_\theta}} \left[ \mathbf{u}(z) \mathbf{k}_f(z)^\top \right] (\mathbf{K} +\sigma^2\mathbf{I})^{-1}\mathbf{Q}
        \\
        &= c_2 E_{z \sim \rho^{\pi_\theta}} \left[ \mathbf{u}(z) \mathbf{u}(z)^\top \right] \mathbf{G}^{-1} \mathbf{U} (\mathbf{K} +\sigma^2\mathbf{I})^{-1}\mathbf{Q}
        \\
        &= c_2 \mathbf{G} \: \mathbf{G}^{-1} \mathbf{U} (c_1\mathbf{K}_s + c_2\mathbf{K}_f +\sigma^2\mathbf{I})^{-1}\mathbf{Q}
        \\
        &= c_2 \mathbf{U} (c_1\mathbf{K}_s + c_2\mathbf{K}_f +\sigma^2\mathbf{I})^{-1}\mathbf{Q}
        \\


        \\
    \displaystyle \mathbf{C}_{\theta}^{BQ} &= \text{Cov} \left[ \nabla_{\theta} J(\theta) | \mathcal{D} \right] = \displaystyle \int dz_1 dz_2 \rho^{\pi_\theta}(z_1) \rho^{\pi_\theta}(z_2) \mathbf{u}(z_1) \text{Cov}\left[ Q_{\pi_\theta}(z_1), Q_{\pi_\theta}(z_2)| \mathcal{D} \right] \mathbf{u}(z_2)^\top
    \\
        &= \mathbb{E}_{z_1,z_2 \sim \rho^{\pi_\theta}} \left[ \mathbf{u}(z_1) \text{Cov}\left[ Q_{\pi_\theta}(z_1), Q_{\pi_\theta}(z_2)| \mathcal{D} \right] \mathbf{u}(z_2)^\top \right]
        \\
        &= \mathbb{E}_{z_1,z_2 \sim \rho^{\pi_\theta}} \left[ \mathbf{u}(z_1) \Big( k(z_1,z_2) - \mathbf{k}(z_1)^\top (\mathbf{K} +\sigma^2\mathbf{I})^{-1} \mathbf{k}(z_2) \Big) \mathbf{u}(z_2)^\top \right]
        \\
        &= \mathbb{E}_{z_1,z_2 \sim \rho^{\pi_\theta}} \left[ \mathbf{u}(z_1) \Big( c_2 k_f(z_1,z_2) - c_2^2 \mathbf{k}_f(z_1)^\top (\mathbf{K} +\sigma^2\mathbf{I})^{-1} \mathbf{k}_f(z_2) \Big) \mathbf{u}(z_2)^\top \right]
        \\
        &= \mathbb{E}_{z_1,z_2 \sim \rho^{\pi_\theta}} \left[ \mathbf{u}(z_1) \mathbf{u}(z_1)^\top \Big( c_2 \mathbf{G}^{-1} - c_2^2 \mathbf{G}^{-1}\mathbf{U} (\mathbf{K} +\sigma^2\mathbf{I})^{-1} \mathbf{U}^\top\mathbf{G}^{-1} \Big) \mathbf{u}(z_2) \mathbf{u}(z_2)^\top \right]
        \\
        &= \mathbb{E}_{z_1 \sim \rho^{\pi_\theta}} [\mathbf{u}(z_1) \mathbf{u}(z_1)^\top] \Big( c_2 \mathbf{G}^{-1}
        - c_2^2 \mathbf{G}^{-1}\mathbf{U} (\mathbf{K} +\sigma^2\mathbf{I})^{-1} \mathbf{U}^\top\mathbf{G}^{-1} \Big) \mathbb{E}_{z_2 \sim \rho^{\pi_\theta}} [\mathbf{u}(z_2) \mathbf{u}(z_2)^\top]
        \\
        &= \mathbf{G} \Big( c_2 \mathbf{G}^{-1} - c_2^2 \mathbf{G}^{-1}\mathbf{U} (\mathbf{K} +\sigma^2\mathbf{I})^{-1} \mathbf{U}^\top\mathbf{G}^{-1} \Big) \mathbf{G}
        \\
        &= c_2 \mathbf{G} - c_2^2\mathbf{U} (c_1\mathbf{K}_s + c_2\mathbf{K}_f +\sigma^2\mathbf{I})^{-1} \mathbf{U}^\top
        \\
        \\
    \end{split}\nonumber
    \end{align}
  </div>

  \begin{equation}\label{eq:GP_grad}
      \displaystyle \mathbf{L}_{\theta}^{BQ} = \displaystyle c_2 \mathbf{U} (c_1\mathbf{K_s} + c_2\mathbf{K_f} +\sigma^2\mathbf{I})^{-1}\mathbf{Q}, \qquad   \displaystyle \mathbf{C}_{\theta}^{BQ} = c_2 \mathbf{G} - c_2^2 \mathbf{U} \left( c_1\mathbf{K_s} + c_2\mathbf{K_f} + \sigma^2 \mathbf{I} \right)^{-1} \mathbf{U}^\top.
  \end{equation}
</p>


<br>
<h1 style="text-align: center; font-variant: small-caps;">Kernel Selection in BQ-PG</h1>
  <div style='float:left; width:40%; padding: 25px;'>
      <img src="https://akella17.github.io/blog/Bayesian-Quadrature-for-Policy-Gradient_test/Brace-yourself-Winter.jpg" class="pub-banner" itemprop="image">
      <!-- <img src="/e-com/images/logo.jpg" style="margin-left:15%;margin-top:5%"/> -->
  </div>
<br>
<p>
  Brace yourself! You have now reached the best part of this post.
</p>


<br>
<h1 style="text-align: center; font-variant: small-caps;">Rough Area</h1>
<p>
Here, $Q_{\pi_\theta}(z)$ is estimated from MC rollouts or an explicit critic network (different from implicit GP critic).

	In particular, this post only covers the fundamentals of BQ-PG, its connection to Monte-Carlo estimation and some other interesting findings. To those less familiar with policy gradient algorithms, I recommend checking-out Lilian Weng's awesome <a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html">blog post</a> (for this post, reading upto <a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#proof-of-policy-gradient-theorem">PG theorem</a> is sufficient). While all this wording might seem confusing, but the underlying idea is very easy to grasp. 
</p>

<p> Policy gradient (PG) algorithms are one of the few reinforcement learning (RL) approaches that offer promising results on high-dimensional environments with a continuous action-space. Their simpliciy and effectiveness makes them quite appealing among other RL approaches. However, the derivation and analysis of PG algorithms is quite different from their deployment in practice, which is closely linked with their biggest criticism, the high-variance problem. More specifically, all the contemporary deep PG algorithms essentially use the policy gradient estimator, Monte-Carlo integration, which we identify as the root cause of this problem. </p>

<p> In this blog, I will introduce you to Bayesian quadrature (BQ), a relatively less popular approach for estimating the policy gradient. While BQ offers significantly better PG estimates for the same number of samples, it scales with a cubic time complexity, which deterred the research along this direction. </p>

<h2>Overview</h2>
<ul>
  <li> In MDP setting, the distribution over state-action pairs is jointly controlled by the policy (known) and the MDP's dynamics (unknown). </li>
  <li> Policy gradient can be expressed as an integration problem, where the integral is under this unknown state-action distribution. </li>
  <li> While it is intractable to evaluate this distribution, sampling from it can be simulated by simply running the policy in the MDP. </li>
<li> We propose a new policy gradient estimator, <strong>deep Bayesian quadrature policy gradient (DBQPG)</strong>, as an alternative to the predominantly used Monte-Carlo estimator. DBQPG provides <strong>more accurate</strong> gradient estimates with a significantly <strong>lower variance</strong>, quantifies the <strong>uncertainty</strong> in policy gradient estimation, and consistently offers a <strong>better performance</strong> for 3 policy gradient algorithms and across 7 MuJoCo environments.</li>
  <li>We also propose a new policy gradient method, <strong>uncertainty aware policy gradient (UAPG)</strong>, that utilizes the quantified estimation uncertainty in DBQPG to compute <strong>reliable policy updates</strong> with <strong>robust step-sizes</strong>.</li>
</ul>

<h2>Motivation</h2>
<p>The universality of policy gradient algorithms, coupled with their simplicity and empirical success, places them amongst the most prominent approaches in the field of reinforcement learning. Infact, there exist more variants of policy gradient algorithms than the number of characters in this blog itself. However, one thing that is common to (almost) all the policy gradient algorithms is its underlying policy gradient estimator: <strong>Monte-Carlo method</strong>.<br>
In its most general form, policy gradient is an integration equation that is (almost always) statistically intractable. Monte-Carlo method offers a computationally-efficient solution for approximating this numerical integration. However, several recent studies highlight that the Monte-Carlo gradient estimation in PG methods is statistical inefficient and undesirably inaccurate. Policy gradient literature comprises of numerous techniques for improving the statistical efficiency of policy gradient estimation, however, (almost) all these methods stick with the standard MC estimator for gradient computation.</p>

<h2>Preliminaries</h2>

<p>Before diving into the policy gradient, I would like to run you through with the problem statement and notation</p>

<h2>Policy Gradients and How to Find Them</h2>



    Reinforcement learning has emerged as the go-to approach for the problem of learning to interact with an unknown environment by a sequence of decisions.

     with the objective of maximizing the expected reward. This framework is particularly appealing for its generality and wide-ranging downstream applications in healthcare, robotic navigation, game-playing, etc. 

    Policy gradient (PG) is a reinforcement learning (RL) approach that directly optimizes the agent’s policies by operating on the gradient of their expected return (Sutton et al., 2000; Baxter & Bartlett, 2000).

    <strong>Note:</strong> Readers are assumed to have basic familiarity with the reinforcement learning framework and policy gradient algorithms. For a quick recap of the reinforcement learning framework and policy gradient algorithms, I would encourage the readers to checkout <a href="https://lilianweng.github.io/lil-log/">Lilian Weng's</a> blogs.

    This blog is written assuming

    Prior policy gradient methods overlook the uncertainty in policy gradient estimation, and naively plug-in stochastic policy gradient estimates in policy gradient algorithms, whose derivation and analysis comprises of access to true gradients.


    <img src="https://akella17.github.io/img/DBQPG.png" class="pub-banner" itemprop="image">
    

    <h3>Outline</h3>
    <p class="pub-abstract" itemprop="text">We study the problem of obtaining accurate policy gradient estimates. This challenge manifests in how best to estimate the policy gradient integral equation using a finite number of samples. Monte-Carlo methods have been the default choice for this purpose, despite suffering from high variance in the gradient estimates. On the other hand, more sample efficient alternatives like Bayesian quadrature methods are less scalable due to their high computational complexity. In this work, we propose deep Bayesian quadrature policy gradient (DBQPG), a computationally efficient high-dimensional generalization of Bayesian quadrature, to estimate the policy gradient integral equation. We show that DBQPG can substitute Monte-Carlo estimation in policy gradient methods, and demonstrate its effectiveness on a set of continuous control benchmarks for robotic locomotion. In comparison to Monte-Carlo estimation, DBQPG provides (i) more accurate gradient estimates with a significantly lower variance, (ii) a consistent improvement in the sample complexity and average return for several on-policy deep policy gradient algorithms, and, (iii) a methodological way to quantify the uncertainty in gradient estimation that can be incorporated to further improve the performance.</p>

    <div class="row">
      <div class="col-sm-1"></div>
      <div class="col-sm-10">
        <div class="row">
          <div class="col-xs-12 col-sm-3 pub-row-heading">Publication</div>
          <div class="col-xs-12 col-sm-9">ArXiv 2020</div>
        </div>
      </div>
      <div class="col-sm-1"></div>
    </div>
    <div class="visible-xs space-below"></div>

    <div class="row">
      <div class="col-sm-1"></div>
      <div class="col-sm-10">
        <div class="row">
          <div class="col-xs-12 col-sm-3 pub-row-heading">Date</div>
          <div class="col-xs-12 col-sm-9" itemprop="datePublished">
            Jun, 2020
          </div>
        </div>
      </div>
      <div class="col-sm-1"></div>
    </div>
    <div class="visible-xs space-below"></div>

    <div class="row" style="padding-top: 10px">
      <div class="col-sm-1"></div>
      <div class="col-sm-10">
        <div class="row">
          <div class="col-xs-12 col-sm-3 pub-row-heading" style="line-height:34px;">Links</div>
          <div class="col-xs-12 col-sm-9">

            
<a class="btn btn-primary btn-outline btn-xs" href="https://arxiv.org/pdf/2006.15637.pdf">
  PDF
</a>
      
      
<a class="btn btn-primary btn-outline btn-xs"
onclick="
    var div = document.getElementById('DBQPG_bib');
    if (div.style.display !== 'none') {
        div.style.display = 'none';
    }
    else {
        div.style.display = 'block';
    }
">
  BibTeX
</a>
<a class="btn btn-primary btn-outline btn-xs" href="https://github.com/Akella17/Deep-Bayesian-Quadrature-Policy-Optimization">
  Code
</a>
      
      
<pre id = "DBQPG_bib", style="display:none">
@article{ravi2020DBQPG,
title={Deep Bayesian Quadrature Policy Optimization},
author={Akella Ravi Tej and Kamyar Azizzadenesheli and Mohammad Ghavamzadeh and Anima Anandkumar and Yisong Yue},
journal={arXiv preprint arXiv:2006.15637},
year={2020}
}
</pre>



          </div>
        </div>
      </div>
      <div class="col-sm-1"></div>
    </div>
    <div class="visible-xs space-below"></div>

    <div class="space-below"></div>

    <div class="article-style">

<h3><b>TL;DR</b></h3>
<ul>
  <li>We propose a new policy gradient estimator, <strong>deep Bayesian quadrature policy gradient (DBQPG)</strong>, as an alternative to the predominantly used Monte-Carlo estimator. Our approach provides <strong>more accurate</strong> gradient estimates with a significantly <strong>lower variance</strong>, quantifies the <strong>uncertainty</strong> in policy gradient estimation, and consistently offers a <strong>better performance</strong> for 3 policy gradient algorithms and across 7 MuJoCo environments.</li>
  <li>We also propose another policy gradient estimator, <strong>uncertainty aware policy gradient (UAPG)</strong>, that utilizes the quantified estimation uncertainty to compute <strong>reliable policy updates</strong> with <strong>robust step-sizes</strong>.</li>
</ul>

<h3><b>Quality of Gradient Estimation</b></h3>

<p><strong>1. Accuracy Plot (Gradient Cosine Similarity) :-</strong>
<img src="/img/DBQPG/accuracy_plot.png" alt="Accuracy_Plot"/></p>

<p><strong>2. Variance Plot (Normalized Gradient Variance) :-</strong>
<img src="/img/DBQPG/variance_plot.png" alt="Variance_Plot"/></p>

<h3 id="b-qualitative-results-b"><b>MuJoCo Experiments</b></h3>

<p><strong>1. Vanilla Policy Gradient :-</strong>
<img src="/img/DBQPG/VanillaPG_plot.png" alt="VanillaPG_Plot"/></p>

<p><strong>2. Natural Policy Gradient :-</strong>
<img src="/img/DBQPG/NPG_plot.png" alt="NPG_Plot"/></p>
      
<p><strong>3. Trust Region Policy Optimization :-</strong>
<img src="/img/DBQPG/TRPO_plot.png" alt="TRPO_Plot"/></p>
</div>

  </div>


</div>
<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2020 Akella Ravi Tej &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
    <script src="/js/jquery-1.12.3.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/hugo-academic.js"></script>
    

    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-53775284-5', 'auto');
        ga('send', 'pageview');

         
        var links = document.querySelectorAll('a');
        Array.prototype.map.call(links, function(item) {
            if (item.host != document.location.host) {
                item.addEventListener('click', function() {
                    var action = item.getAttribute('data-action') || 'follow';
                    ga('send', 'event', 'outbound', action, item.href);
                });
            }
        });
    </script>
    

    
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }});
    </script>
    <script async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
    

  </body>
</html>
